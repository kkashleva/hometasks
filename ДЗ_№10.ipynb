{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "–î–ó ‚Ññ10",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkashleva/hometasks/blob/main/%D0%94%D0%97_%E2%84%9610.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**–ó–∞–¥–∞–Ω–∏–µ 1 (8 –±–∞–ª–ª–æ–≤).**\n",
        "\n",
        "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥–æ–æ–±—É—á–∞—Ç—å GPT –Ω–∞ –∫–∞–∫–æ–º-—Ç–æ –¥—Ä—É–≥–æ–º —Ç–µ–∫—Å—Ç–µ (–º–æ–∂–µ—Ç–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ª—é–±—ã–µ —Å—Ç–∏—Ö–∏ –∏–ª–∏ –∫–∞–∫–∏–µ-—Ç–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –≤–µ—â–∏ –≤—Ä–æ–¥–µ –∞–Ω–µ–∫–¥–æ—Ç–æ–≤, —Ç–µ–æ—Ä–∏–π –∑–∞–≥–æ–≤–æ—Ä–æ–≤, –ø–æ—Å—Ç–æ–≤ –≤ –ø–æ–º–æ–µ—á–Ω—ã—Ö —Ç–µ–ª–µ–≥—Ä–∞–º –∫–∞–Ω–∞–ª–∞—Ö, —Ç–µ–∫—Å—Ç–æ–≤ –∂—É—Ä–Ω–∞–ª–∏—Å—Ç–æ–≤ –∏ –°–ú–ò —Å –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—ã–º —Å—Ç–∏–ª–µ–º). –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (beam search, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞, top_k –∏ —Ç–ø). –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –≤ —Ç–µ—Ç—Ä–∞–¥–∫–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–∏—Ö —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤\n",
        "\n"
      ],
      "metadata": {
        "id": "F7LbbTP15xUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "CNKpIGwdFJx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad95195e-faca-4e64-d8a5-64599e0ff837"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.2 MB 7.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 84 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 596 kB 71.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.6 MB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVPq82s-5qE1",
        "outputId": "70ea4fc9-4d01-4819-a2b5-22565b83f432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/sberbank-ai/rugpt3large_based_on_gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/f4f35cc129ac1e5d04519f354f7b5a34b169846c4fdf0185b6a16b13b9ebcc65.c483bc3440d25937fdac74506b73b76ee6e67f778a804756214363fc2a1a66ef\n",
            "loading file https://huggingface.co/sberbank-ai/rugpt3large_based_on_gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/8e2c17ed6a68f33c3913de0a03b9b899239c05c0a06f1bde745a0698dc01cf73.7362c0dbb32f750eeea5a5b93bbd0c6876eac41453369265d5a49df1c9142b6f\n",
            "loading file https://huggingface.co/sberbank-ai/rugpt3large_based_on_gpt2/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/sberbank-ai/rugpt3large_based_on_gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/sberbank-ai/rugpt3large_based_on_gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/sberbank-ai/rugpt3large_based_on_gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a4aea75af518e10b02aac733b25c92d979cec74d3b48edb877e13d5e4d4792f.aa4141df4e4cfca5435e8aa371aea7f575eb2d0767c1395e0d4cd6209796a705\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"sberbank-ai/rugpt3large_based_on_gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 1536,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/sberbank-ai/rugpt3large_based_on_gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a4aea75af518e10b02aac733b25c92d979cec74d3b48edb877e13d5e4d4792f.aa4141df4e4cfca5435e8aa371aea7f575eb2d0767c1395e0d4cd6209796a705\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 1536,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/sberbank-ai/rugpt3large_based_on_gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7cf248f6c39196b677fb5b9b9ee8da3ded29f363018f4ccb1d0605721c719f4c.75e651cd6468a93822a2ca422a07b480dacd0c2d13ac194fdf771f768e6a8447\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at sberbank-ai/rugpt3large_based_on_gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "DEVICE = torch.device(\"cuda:0\")\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å ruGPT –æ—Ç —Å–±–µ—Ä–∞\n",
        "model_name_or_path = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '–ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ, —Å –µ–≥–æ —à—É—Ç–ª–∏–≤–æ–π —Ü–∏—Ç–∞—Ç–æ–π –∏–∑ –†–æ—Å—Å–∏–Ω–∏ –≤ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏ –∏ –∏—Å–∫–∞–∂–µ–Ω–Ω–æ–π —Ü–∏—Ç–∞—Ç–æ–π –∏–∑ –í–∞–≥–Ω–µ—Ä–∞ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π, \\\n",
        "–≤—ã–∑–≤–∞–ª–æ –æ–∑–∞–¥–∞—á–µ–Ω–Ω—É—é —Ä–µ–∞–∫—Ü–∏—é —É –≤—Å–µ—Ö. –ù–µ–ª—å–∑—è —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ–±—ã –æ–Ω–æ –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å'\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)"
      ],
      "metadata": {
        "id": "lmQRQp4pPTCa"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Argmax\n"
      ],
      "metadata": {
        "id": "d9kdoJKWLpIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out = model.generate(input_ids, do_sample=True, max_length=100)\n",
        "\n",
        "\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnnm9ILzmHYe",
        "outputId": "5f6c20a5-183e-45c2-d0a2-0d4583aebfb7"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "–ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ, —Å –µ–≥–æ —à—É—Ç–ª–∏–≤–æ–π —Ü–∏—Ç–∞—Ç–æ–π –∏–∑ –†–æ—Å—Å–∏–Ω–∏ –≤ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏ –∏ –∏—Å–∫–∞–∂–µ–Ω–Ω–æ–π —Ü–∏—Ç–∞—Ç–æ–π –∏–∑ –í–∞–≥–Ω–µ—Ä–∞ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π, –≤—ã–∑–≤–∞–ª–æ –æ–∑–∞–¥–∞—á–µ–Ω–Ω—É—é —Ä–µ–∞–∫—Ü–∏—é —É –≤—Å–µ—Ö. –ù–µ–ª—å–∑—è —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ–±—ã –æ–Ω–æ –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å –≤—Å–µ–º. –î–∞–∂–µ –º–Ω–µ, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–≤—ã–∫ —á–∏—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–µ–µ. (–ù–æ –¥–∞–∂–µ —è –ø–æ–Ω—è–ª, —á—Ç–æ —ç—Ç–∞ –∫–Ω–∏–≥–∞ –Ω–µ –º–æ–∂–µ—Ç –Ω–µ –ø–æ–Ω—Ä–∞–≤–∏—Ç—Å—è!) –ü–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∞ ‚Äî –æ –ª—é–±–≤–∏. –û —Å–∞–º–æ–π –≥–ª–∞–≤–Ω–æ–π, —Å–∞–º–æ–π –≥–ª–∞–≤–Ω–æ–π –∏ —Å–∞–º–æ–π –Ω–µ–≤—ã–Ω–æ—Å–∏–º–æ–π –ª—é–±–≤–∏ ‚Äî –∫–æ—Ç–æ—Ä—É—é –Ω–µ –∫–∞–∂–¥–æ–º—É —É–¥–∞–µ—Ç—Å—è —Å–æ–∑–¥–∞—Ç—å.<s>\n",
            "–ó–æ–Ω–∞ –ø–æ–≤—ã—à–µ–Ω–Ω–æ–π –æ–ø–∞—Å–Ω–æ—Å—Ç–∏ \n",
            "\n",
            "–ö —Å–ª–æ–≤—É\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search\n"
      ],
      "metadata": {
        "id": "el-w_tHjmKiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# beam search —É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –≤ hg –ø–æ—ç—Ç–æ–º—É –Ω—É–∂–Ω–æ —Ç–æ–ª—å–∫–æ –∑–∞–¥–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä num_beams\n",
        "out = model.generate(input_ids, do_sample=True, num_beams=10, max_length=100)\n",
        "\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoRylMtVrWrW",
        "outputId": "423a79da-fb19-4e00-fab6-0328b4fa0ac6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "–ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ, —Å –µ–≥–æ —à—É—Ç–ª–∏–≤–æ–π —Ü–∏—Ç–∞—Ç–æ–π –∏–∑ –†–æ—Å—Å–∏–Ω–∏ –≤ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏ –∏ –∏—Å–∫–∞–∂–µ–Ω–Ω–æ–π —Ü–∏—Ç–∞—Ç–æ–π –∏–∑ –í–∞–≥–Ω–µ—Ä–∞ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π, –≤—ã–∑–≤–∞–ª–æ –æ–∑–∞–¥–∞—á–µ–Ω–Ω—É—é —Ä–µ–∞–∫—Ü–∏—é —É –≤—Å–µ—Ö. –ù–µ–ª—å–∑—è —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ–±—ã –æ–Ω–æ –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å –≤—Å–µ–º, –Ω–æ –≤ —Ü–µ–ª–æ–º –æ–Ω–æ –±—ã–ª–æ –≤–æ—Å–ø—Ä–∏–Ω—è—Ç–æ –∫–∞–∫ –∫–æ–º–ø–ª–∏–º–µ–Ω—Ç.\n",
            "\n",
            "‚Äî¬†–ù—É –∏ —á—Ç–æ?¬†‚Äî —Å–ø—Ä–æ—Å–∏–ª –∫—Ç–æ-—Ç–æ.\n",
            "\n",
            "‚Äî¬†–ê —Ç–æ,¬†‚Äî —Å–∫–∞–∑–∞–ª –∫—Ç–æ-—Ç–æ,¬†‚Äî —á—Ç–æ —ç—Ç–æ –Ω–µ –∫–æ–º–ø–ª–∏–º–µ–Ω—Ç.\n",
            "\n",
            "‚Äî¬†–ê —á—Ç–æ?¬†‚Äî\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π\n"
      ],
      "metadata": {
        "id": "Qej9EsZJrcL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out = model.generate(input_ids, do_sample=True, temperature=0.5, max_length=100)\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhYQ2K9ysyit",
        "outputId": "c798a0de-4974-4e46-c2ff-c6f97ca8597b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "–ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ, —Å –µ–≥–æ —à—É—Ç–ª–∏–≤–æ–π —Ü–∏—Ç–∞—Ç–æ–π –∏–∑ –†–æ—Å—Å–∏–Ω–∏ –≤ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏ –∏ –∏—Å–∫–∞–∂–µ–Ω–Ω–æ–π —Ü–∏—Ç–∞—Ç–æ–π –∏–∑ –í–∞–≥–Ω–µ—Ä–∞ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π, –≤—ã–∑–≤–∞–ª–æ –æ–∑–∞–¥–∞—á–µ–Ω–Ω—É—é —Ä–µ–∞–∫—Ü–∏—é —É –≤—Å–µ—Ö. –ù–µ–ª—å–∑—è —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ–±—ã –æ–Ω–æ –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å –ø—É–±–ª–∏–∫–µ. –û–¥–Ω–∞–∫–æ, –ø–æ—Å–ª–µ —Ç–æ–≥–æ –∫–∞–∫ –æ–Ω–æ –±—ã–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –ø—É–±–ª–∏–∫–µ, –µ–≥–æ —Å—Ç–∞–ª–∏ –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞—Ç—å –≤—Å–µ—Ä—å–µ–∑.\n",
            "\n",
            "–í –Ω–∞—á–∞–ª–µ 1960-—Ö –≥–æ–¥–æ–≤ –∫–æ–º–ø–æ–∑–∏—Ç–æ—Ä –∏ –¥–∏—Ä–∏–∂–µ—Ä –î–∂–æ—Ä–¥–∂ –ì–µ—Ä—à–≤–∏–Ω, –∫–æ—Ç–æ—Ä—ã–π –±—ã–ª –∑–Ω–∞–∫–æ–º —Å –†–æ—Å—Å–∏–Ω–∏, –∞ —Ç–∞–∫–∂–µ —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ –¥—Ä—É–≥–∏–º–∏ –∫–æ–º–ø–æ–∑–∏—Ç–æ—Ä–∞–º–∏ –ø—Ä–æ—à–ª–æ–≥–æ, –Ω–∞–ø–∏—Å–∞–ª –¥–ª—è –Ω–µ–≥–æ –æ–ø–µ—Ä—É ¬´–§–∏–¥–µ–ª–∏–æ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=100)\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajqXyOGFOkwa",
        "outputId": "65ac111e-9bae-4904-90fd-b7ccf19f1400"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "–ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ, —Å –µ–≥–æ —à—É—Ç–ª–∏–≤–æ–π —Ü–∏—Ç–∞—Ç–æ–π –∏–∑ –†–æ—Å—Å–∏–Ω–∏ –≤ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏ –∏ –∏—Å–∫–∞–∂–µ–Ω–Ω–æ–π —Ü–∏—Ç–∞—Ç–æ–π –∏–∑ –í–∞–≥–Ω–µ—Ä–∞ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π, –≤—ã–∑–≤–∞–ª–æ –æ–∑–∞–¥–∞—á–µ–Ω–Ω—É—é —Ä–µ–∞–∫—Ü–∏—é —É –≤—Å–µ—Ö. –ù–µ–ª—å–∑—è —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ–±—ã –æ–Ω–æ –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å –±–æ–ª—å—à–∏–Ω—Å—Ç–≤—É –∑—Ä–∏—Ç–µ–ª–µ–π, —Ö–æ—Ç—è –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≤–æ—Å–ø—Ä–∏–Ω—è–ª–∏ —ç—Ç–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –∫–∞–∫ –ø–∞—Ä–æ–¥–∏—é. –ù–æ –≤ —Ü–µ–ª–æ–º –æ–Ω–æ –±—ã–ª–æ –≤–æ—Å–ø—Ä–∏–Ω—è—Ç–æ –±–ª–∞–≥–æ—Å–∫–ª–æ–Ω–Ω–æ, —Ö–æ—Ç—è –∏ –Ω–µ –±–µ–∑ –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ —Å–∫–µ–ø—Å–∏—Å–∞.\n",
            "\n",
            "–ü—Ä–∏–º–µ—á–∞–Ω–∏—è –∫ ¬´–°–æ–Ω–∞—Ç—É –¥–ª—è —Å–∫—Ä–∏–ø–∫–∏ —Å –æ—Ä–∫–µ—Å—Ç—Ä–æ–º¬ª\n",
            "\n",
            "1\n",
            "\n",
            "–ó–¥–µ—Å—å –∏ –¥–∞–ª–µ–µ –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –¥–ª—è\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**top_k**"
      ],
      "metadata": {
        "id": "j0I1xXhes3eB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out = model.generate(input_ids, \n",
        "                     do_sample=True,\n",
        "                     temperature=1.0,\n",
        "                     top_k=20,\n",
        "                     max_length=100,\n",
        "                    )\n",
        "\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8yvVnTctmYn",
        "outputId": "d83025e5-ad4c-462e-c288-986489975c1b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "–ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ, —Å –µ–≥–æ —à—É—Ç–ª–∏–≤–æ–π —Ü–∏—Ç–∞—Ç–æ–π –∏–∑ –†–æ—Å—Å–∏–Ω–∏ –≤ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏ –∏ –∏—Å–∫–∞–∂–µ–Ω–Ω–æ–π —Ü–∏—Ç–∞—Ç–æ–π –∏–∑ –í–∞–≥–Ω–µ—Ä–∞ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π, –≤—ã–∑–≤–∞–ª–æ –æ–∑–∞–¥–∞—á–µ–Ω–Ω—É—é —Ä–µ–∞–∫—Ü–∏—é —É –≤—Å–µ—Ö. –ù–µ–ª—å–∑—è —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ–±—ã –æ–Ω–æ –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å –≤—Å–µ–º, –æ–¥–Ω–∞–∫–æ –∫—Ä–∏—Ç–∏–∫–∏ –Ω–µ –ø—Ä–µ–º–∏–Ω—É–ª–∏ –≤—ã—Å–º–µ—è—Ç—å \"–ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π —Å—Ç–∏–ª—å\", –Ω–µ –ø–æ–Ω–∏–º–∞—è, —á—Ç–æ, –≤–æ-–ø–µ—Ä–≤—ã—Ö, –≤ –†–æ—Å—Å–∏–∏ —ç—Ç–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –Ω–∞–ø–∏—Å–∞–Ω–æ –≤ —Ç–æ–º –≤–∏–¥–µ, –≤ –∫–∞–∫–æ–º –æ–Ω–æ –µ—Å—Ç—å, –∏, –≤–æ-–≤—Ç–æ—Ä—ã—Ö, –≤ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ —Å–ª–æ–≤–∞ \"—Ä–æ–º–∞–Ω\", \"–∏—Å—Ç–æ—Ä–∏—è\" –∏ \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –î–æ–æ–±—É—á–µ–Ω–∏–µ GPT\n",
        "\n"
      ],
      "metadata": {
        "id": "QByyuFm4gy_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name_or_path, use_cache=False).to(DEVICE)"
      ],
      "metadata": {
        "id": "Ha9HPro0eQWm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "474bc1c0-ee26-40fb-d682-a59c7dde06b1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/1b36eeb1fd7b3a6ec11bf46bde2c38e7e68f71ec774694b9e886c86001aab35d.c483bc3440d25937fdac74506b73b76ee6e67f778a804756214363fc2a1a66ef\n",
            "loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/479aa59074c4dcd4c36106252da033d03bc92e3010947ce1d3714de224c2af1f.7362c0dbb32f750eeea5a5b93bbd0c6876eac41453369265d5a49df1c9142b6f\n",
            "loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06f48b6b3173390d047e15d691fda67ae4ea7733a5eea4b6e0115f5099c4e700.b5cdfa39c63384f94159c36bc9042660c747cea5cf520b43d543bd2c68b3164d\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06f48b6b3173390d047e15d691fda67ae4ea7733a5eea4b6e0115f5099c4e700.b5cdfa39c63384f94159c36bc9042660c747cea5cf520b43d543bd2c68b3164d\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/df2b64a4c86a349ba84354d85b7117b106f2b87085c9bb54cde70d3751907c45.4e3da19dd8adaa6d6a9804bfd45d2dcf17ba544de445847443ef1816bfa3d693\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at sberbank-ai/rugpt3small_based_on_gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–°—Ç–∞—Ç—å—è –æ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π –º—É–∑—ã–∫–µ ( https://www.kommersant.ru/doc/5358770?from=main)"
      ],
      "metadata": {
        "id": "6Ip1xq3So0B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "–ö–æ–Ω—Ü–µ—Ä—Ç 8 —è–Ω–≤–∞—Ä—è 1972 –≥–æ–¥–∞ –≤ –ë–æ–ª—å—à–æ–º –∑–∞–ª–µ –ú–æ—Å–∫–æ–≤—Å–∫–æ–π –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–æ—Ä–∏–∏ —Å—Ç–∞–ª –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º –Ω–µ —Ç–æ–ª—å–∫–æ –∏–∑-–∑–∞ –º–∏—Ä–æ–≤–æ–π –ø—Ä–µ–º—å–µ—Ä—ã –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–∏–º—Ñ–æ–Ω–∏–∏ –®–æ—Å—Ç–∞–∫–æ–≤–∏—á–∞. –í –ø–µ—Ä–≤–æ–º –æ—Ç–¥–µ–ª–µ–Ω–∏–∏ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —Ä–∞–∑ –≤—ã—Å—Ç—É–ø–∏–ª–∏ –≤ –¥—É—ç—Ç–µ –Ω–µ–¥–∞–≤–Ω–∏–µ –ø–æ–±–µ–¥–∏—Ç–µ–ª–∏ –∫–æ–Ω–∫—É—Ä—Å–∞ –ß–∞–π–∫–æ–≤—Å–∫–æ–≥–æ ‚Äî —Å–æ–≤—Å–µ–º –º–æ–ª–æ–¥—ã–µ –ì–∏–¥–æ–Ω –ö—Ä–µ–º–µ—Ä –∏ –í–ª–∞–¥–∏–º–∏—Ä –ö—Ä–∞–π–Ω–µ–≤, —Å—ã–≥—Ä–∞–≤—à–∏–µ —Ä–µ–¥–∫–æ—Å—Ç–Ω—ã–π –ö–æ–Ω—Ü–µ—Ä—Ç –¥–ª—è —Å–∫—Ä–∏–ø–∫–∏, –∫–ª–∞–≤–∏—Ä–∞ –∏ —Å—Ç—Ä—É–Ω–Ω—ã—Ö –ì–∞–π–¥–Ω–∞. –ë–æ–ª—å—à–∏–º —Å–∏–º—Ñ–æ–Ω–∏—á–µ—Å–∫–∏–º –æ—Ä–∫–µ—Å—Ç—Ä–æ–º –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Ç–µ–ª–µ–≤–∏–¥–µ–Ω–∏—è –∏ –í—Å–µ—Å–æ—é–∑–Ω–æ–≥–æ —Ä–∞–¥–∏–æ –¥–∏—Ä–∏–∂–∏—Ä–æ–≤–∞–ª –ú–∞–∫—Å–∏–º –®–æ—Å—Ç–∞–∫–æ–≤–∏—á. –ö–∞–∫ –≤—Å–ø–æ–º–∏–Ω–∞–µ—Ç –º—É–∑—ã–∫–æ–≤–µ–¥ –†–∏—á–∞—Ä–¥ –¢–∞—Ä—É—Å–∫–∏–Ω, –≤ —Ç–µ –≥–æ–¥—ã —Å—Ç–∞–∂–µ—Ä –ú–æ—Å–∫–æ–≤—Å–∫–æ–π –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–æ—Ä–∏–∏, ¬´–ø–æ—Ç—Ä—è—Å–µ–Ω–∏–µ, –∏—Å–ø—ã—Ç–∞–Ω–Ω–æ–µ –º–Ω–æ—é –≤ —Ç–æ—Ç –≤–µ—á–µ—Ä, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏–ª–æ –≤—Å–µ, —á—Ç–æ –º–Ω–µ –¥–æ–≤–µ–ª–æ—Å—å –∏—Å–ø—ã—Ç—ã–≤–∞—Ç—å –Ω–∞ –∫–æ–Ω—Ü–µ—Ä—Ç–∞—Ö –ø—Ä–µ–∂–¥–µ –∏ –ø–æ—Å–ª–µ¬ª. –ó–∞—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –¥—É—Ö –ø—Ä–∏ –æ–¥–Ω–æ–π –º—ã—Å–ª–∏ –æ —Ç–∞–∫–æ–º –∫–æ–Ω—Ü–µ—Ä—Ç–µ, —Å–æ—Å—Ç–æ—è–≤—à–µ–º—Å—è –≤—Ä–æ–¥–µ –±—ã –∏ –Ω–µ —Ç–∞–∫ –¥–∞–≤–Ω–æ,‚Äî –∏ –Ω–µ –∑—Ä—è –≤ –ü–µ—Ä–º–∏ —Ä–µ—à–∏–ª–∏ –æ –Ω–µ–º –Ω–∞–ø–æ–º–Ω–∏—Ç—å —Ö–æ—Ç—è –±—ã –∏ –Ω–µ –¥–µ–Ω—å –≤ –¥–µ–Ω—å.\n",
        "\n",
        "–ì–ª–∞–≤–Ω—ã–º –≥–µ—Ä–æ–µ–º —Ç–µ–ø–µ—Ä–µ—à–Ω–µ–≥–æ –≤–µ—á–µ—Ä–∞, –µ—Å–ª–∏ –Ω–µ —Å—á–∏—Ç–∞—Ç—å –®–æ—Å—Ç–∞–∫–æ–≤–∏—á–∞, –æ–∂–∏–¥–∞–µ–º–æ —Å—Ç–∞–ª –§–∏–ª–∏–ø–ø –ß–∏–∂–µ–≤—Å–∫–∏–π. –ú–µ—Å—è—Ü –Ω–∞–∑–∞–¥ –æ–Ω –ø–æ–ª—É—á–∏–ª —Å–≤–æ—é –≤—Ç–æ—Ä—É—é ¬´–ó–æ–ª–æ—Ç—É—é –º–∞—Å–∫—É¬ª –≤ –Ω–æ–º–∏–Ω–∞—Ü–∏–∏ ¬´–õ—É—á—à–∞—è —Ä–∞–±–æ—Ç–∞ –¥–∏—Ä–∏–∂–µ—Ä–∞¬ª –∑–∞ –ø–æ—Å—Ç–∞–Ω–æ–≤–∫—É ¬´–ö–∞—Ä–º–µ–Ω¬ª –≤ –ü–µ—Ä–º—Å–∫–æ–π –æ–ø–µ—Ä–µ –∏ —Ç–µ–ø–µ—Ä—å –≤–µ—Ä–Ω—É–ª—Å—è —Å—é–¥–∞ —É–∂–µ –∫–∞–∫ –¥–∏—Ä–∏–∂–µ—Ä —Å–∏–º—Ñ–æ–Ω–∏—á–µ—Å–∫–∏–π. –§–∏–ª–∏–ø–ø –Ω–µ —Ç–∞–∫ —á–∞—Å—Ç–æ –≤—ã—Å—Ç—É–ø–∞–µ—Ç –≤ —ç—Ç–æ–º –∞–º–ø–ª—É–∞, –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞—è –∏—Å–ø–æ–ª–Ω—è—Ç—å —Å—Ç–∞—Ä–∏–Ω–Ω—É—é –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –º—É–∑—ã–∫—É –ª–∏–±–æ —Å–æ —Å–≤–æ–∏–º –∞–Ω—Å–∞–º–±–ª–µ–º Questa Musica, –ª–∏–±–æ —Å –¥—Ä—É–≥–∏–º–∏ –ø—Ä–æ—Ñ–∏–ª—å–Ω—ã–º–∏ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–∞–º–∏. –ú–µ–∂–¥—É —Ç–µ–º –µ–≥–æ –Ω–µ–¥–∞–≤–Ω–∏–π –∫–æ–Ω—Ü–µ—Ä—Ç —Å –ê–°–û –ú–æ—Å–∫–æ–≤—Å–∫–æ–π —Ñ–∏–ª–∞—Ä–º–æ–Ω–∏–∏ –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ —Å–∏–º—Ñ–æ–Ω–∏—á–µ—Å–∫–∏–π –æ—Ä–∫–µ—Å—Ç—Ä –ø–æ–¥–≤–ª–∞—Å—Ç–µ–Ω –ß–∏–∂–µ–≤—Å–∫–æ–º—É –Ω–∏—á—É—Ç—å –Ω–µ –º–µ–Ω—å—à–µ, —á–µ–º –Ω–µ–±–æ–ª—å—à–∏–µ –º–æ–±–∏–ª—å–Ω—ã–µ —Å–æ—Å—Ç–∞–≤—ã. –ü–æ—Å–ª–µ–¥–Ω—é—é —Å–∏–º—Ñ–æ–Ω–∏—é –®–æ—Å—Ç–∞–∫–æ–≤–∏—á–∞, –ø–æ–ª–Ω—É—é —Ü–∏—Ç–∞—Ç, –∞–ª–ª—é–∑–∏–π, —à–∏—Ñ—Ä–æ–≤, –ø–æ–¥—Ç–µ–∫—Å—Ç–æ–≤, —á–∞—Å—Ç–æ –Ω–µ –ø–æ–¥–¥–∞—é—â–∏—Ö—Å—è –æ—á–µ–≤–∏–¥–Ω—ã–º —Ç–æ–ª–∫–æ–≤–∞–Ω–∏—è–º, –≤—Å–µ–≥–¥–∞ –µ—Å—Ç—å —Ä–∏—Å–∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫ —Å–µ—Ä–∏—é —ç–ø–∏–∑–æ–¥–æ–≤, –∞ –Ω–µ –∫–∞–∫ —Ü–µ–ª–æ–µ; –ß–∏–∂–µ–≤—Å–∫–∏–π —Å –æ—Ä–∫–µ—Å—Ç—Ä–æ–º —Ç–µ–∞—Ç—Ä–∞ —ç—Ç–æ–π –æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Å—á–∞—Å—Ç–ª–∏–≤–æ –∏–∑–±–µ–∂–∞–ª–∏.\n",
        "\n",
        "–î–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ, —Ü–∏—Ç–∞—Ç—ã –∏–∑ ¬´–í–∏–ª—å–≥–µ–ª—å–º–∞ –¢–µ–ª–ª—è¬ª –∏ ¬´–ì–∏–±–µ–ª–∏ –±–æ–≥–æ–≤¬ª –∏ —Å–µ–≥–æ–¥–Ω—è –º–æ–≥—É—Ç —Å–º—É—Ç–∏—Ç—å —Å–≤–æ–µ–π –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç—å—é ‚Äî –º–æ–∂–Ω–æ —Å–µ–±–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å, –∫–∞–∫ —É–¥–∏–≤–ª—è–ª–∏—Å—å –ø–µ—Ä–≤—ã–µ —Å–ª—É—à–∞—Ç–µ–ª–∏. –í –ü—è—Ç–Ω–∞–¥—Ü–∞—Ç–æ–π –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ –ø–æ—á—Ç–∏ –≤—Å—ë: –∫–∞–∫ –Ω–∏ –∏—â–∏ —Å–ª–æ–≤–∞, –æ—Ç –Ω–∏—Ö —É—Å–∫–æ–ª—å–∑–∞—é—Ç –∏ –æ—Ç–∫—Ä—ã–≤–∞—é—â–µ–µ —Å–∏–º—Ñ–æ–Ω–∏—é —Å–æ–ª–æ —Ñ–ª–µ–π—Ç—ã (–ò—Ä–∏–Ω–∞ –ú–∞—Ä—Ü–∏–Ω–∫–µ–≤–∏—á) ‚Äî –Ω–∞ –ø–µ—Ä–≤—ã–π –≤–∑–≥–ª—è–¥ –±–µ–∑–∑–∞–±–æ—Ç–Ω–æ–µ, –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –Ω–∏—á—É—Ç—å, –∏ –¥–∏–∞–ª–æ–≥–∏ –º–µ–¥–Ω—ã—Ö –¥—É—Ö–æ–≤—ã—Ö —Å –≤–∏–æ–ª–æ–Ω—á–µ–ª—å—é —Å–æ–ª–æ –≤–æ –≤—Ç–æ—Ä–æ–π —á–∞—Å—Ç–∏ (–†–æ–º–∞–Ω –ï—Ñ–∏–º–æ–≤) ‚Äî —Ç–æ –ª–∏ –∑–ª–æ–≤–µ—â–∏–µ, —Ç–æ –ª–∏ —Ç–æ—Å–∫—É—é—â–∏–µ, –∏ –º–Ω–æ–≥–æ–µ –¥—Ä—É–≥–æ–µ. –†–∞–∑–≤–µ —á—Ç–æ —Ä–µ–º–∏–Ω–∏—Å—Ü–µ–Ω—Ü–∏—è –∏–∑ –∑–Ω–∞–º–µ–Ω–∏—Ç–æ–π ¬´—Ç–µ–º—ã –Ω–∞—à–µ—Å—Ç–≤–∏—è¬ª –°–µ–¥—å–º–æ–π —Å–∏–º—Ñ–æ–Ω–∏–∏ –≤ —Ñ–∏–Ω–∞–ª–µ –∏–º–µ–µ—Ç —è–≤–Ω—ã–π –æ—Ç—Ç–µ–Ω–æ–∫ –∞–≤—Ç–æ—ç–ø–∏—Ç–∞—Ñ–∏–∏, —á—Ç–æ —Å–µ–≥–æ–¥–Ω—è –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞–µ—Ç—Å—è —Å–∫–æ—Ä–µ–µ –∫–∞–∫ —ç–ø–∏—Ç–∞—Ñ–∏—è –≤—Å–µ–º—É –º–∏—Ä—É. –ò–ª–∏, –≤–æ–∑–º–æ–∂–Ω–æ, –∫–∞–∫ –æ–±–µ—â–∞–Ω–∏–µ –º–∏—Ä–∞.\n",
        "\n",
        "–ü–µ—Ä–µ–¥ –∞–Ω—Ç—Ä–∞–∫—Ç–æ–º –∏—Å–ø–æ–ª–Ω–∏–ª–∏ –º–∏—Ä–æ–≤—É—é –ø—Ä–µ–º—å–µ—Ä—É ‚Äî –§–æ—Ä—Ç–µ–ø–∏–∞–Ω–Ω—É—é —Å–æ–Ω–∞—Ç—É ‚Ññ31 –ë–µ—Ç—Ö–æ–≤–µ–Ω–∞ –≤ –æ—Ä–∫–µ—Å—Ç—Ä–æ–≤–∫–µ –ê–ª–µ–∫—Å–µ—è –°—ã—Å–æ–µ–≤–∞. –û—Ç –æ–¥–Ω–æ–≥–æ –∏–∑ —Å–∞–º—ã—Ö —è—Ä–∫–∏—Ö –∏ –±–µ—Å–∫–æ–º–ø—Ä–æ–º–∏—Å—Å–Ω—ã—Ö, —á—Ç–æ–±—ã –Ω–µ —Å–∫–∞–∑–∞—Ç—å —Ä–∞–¥–∏–∫–∞–ª—å–Ω—ã—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∫–æ–º–ø–æ–∑–∏—Ç–æ—Ä–æ–≤ –±—ã–ª–æ –≤–ø–æ—Ä—É –∂–¥–∞—Ç—å –µ—Å–ª–∏ –Ω–µ —Ä–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏, —Ç–æ –∫–∞–∫ –º–∏–Ω–∏–º—É–º –ø–æ–ª–µ–º–∏–∫–∏ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º. –û–¥–Ω–∞–∫–æ —Ä–µ—á—å –µ—â–µ –æ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–æ–π —Ä–∞–±–æ—Ç–µ –°—ã—Å–æ–µ–≤–∞ —Å–µ—Ä–µ–¥–∏–Ω—ã 2000-—Ö, –∑–≤—É—á–∞—â–µ–π –≤–ø–æ–ª–Ω–µ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏ —Ä—è–¥–æ–º —Å –µ–≥–æ –∑—Ä–µ–ª—ã–º–∏ —Å–æ—á–∏–Ω–µ–Ω–∏—è–º–∏; –≤–ø—Ä–æ—á–µ–º, –Ω–∞ –∏—Ö —Ñ–æ–Ω–µ —Ç–∞–∫–∞—è –ø—Ä–µ–º—å–µ—Ä–∞ ‚Äî —Ö–æ–¥ —Ç–æ–∂–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–∞–¥–∏–∫–∞–ª—å–Ω—ã–π. ¬´–ë–µ—Ç—Ö–æ–≤–µ–Ω –ø–∏—à–µ—Ç –¥–ª—è —Ñ–æ—Ä—Ç–µ–ø–∏–∞–Ω–æ —Å–∏–º—Ñ–æ–Ω–∏—á–µ—Å–∫—É—é –º—É–∑—ã–∫—É¬ª ‚Äî —Å —ç—Ç–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ –ê–ª–µ–∫—Å–µ—è –º–æ–∂–Ω–æ —Å–æ–≥–ª–∞—à–∞—Ç—å—Å—è –∏–ª–∏ –Ω–µ—Ç, –Ω–æ –µ–≥–æ –æ—Ä–∫–µ—Å—Ç—Ä–æ–≤–∫–∞ –æ—á–µ–Ω—å —É–±–µ–¥–∏—Ç–µ–ª—å–Ω–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –ø–µ—Ä–µ–∫–ª–∏—á–∫–∏ —Å—Ç—Ä—É–Ω–Ω—ã—Ö –∏ –¥—É—Ö–æ–≤—ã—Ö. –ê –Ω–∞—á–∞–ª–æ —Ç—Ä–µ—Ç—å–µ–π —á–∞—Å—Ç–∏ —Å –º—Ä–∞—á–Ω—ã–º–∏ –º–µ–¥–Ω—ã–º–∏ –¥—É—Ö–æ–≤—ã–º–∏ –∏ –≤–∏–æ–ª–æ–Ω—á–µ–ª—å–Ω—ã–º —Å–æ–ª–æ –∑–≤—É—á–∏—Ç —Ç–∞–∫, –±—É–¥—Ç–æ –ø–µ—Ä–µ–ª–æ–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–≤–∞–ª–æ—Å—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –≤–µ—á–µ—Ä–∞ —Å –ü—è—Ç–Ω–∞–¥—Ü–∞—Ç–æ–π —Å–∏–º—Ñ–æ–Ω–∏–µ–π –®–æ—Å—Ç–∞–∫–æ–≤–∏—á–∞.\n",
        "\n",
        "–û—Ç–∫—Ä—ã–ª –ø—Ä–æ–≥—Ä–∞–º–º—É –ö–æ–Ω—Ç—Ä–∞–ø—É–Ω–∫—Ç XIX –∏–∑ ¬´–ò—Å–∫—É—Å—Å—Ç–≤–∞ —Ñ—É–≥–∏¬ª –ë–∞—Ö–∞ –≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –õ—É—á–∞–Ω–æ –ë–µ—Ä–∏–æ –¥–ª—è 23 –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–π. –ï—Å–ª–∏ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –°—ã—Å–æ–µ–≤–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–ø–æ–ª–Ω–µ –±–µ—Ç—Ö–æ–≤–µ–Ω—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤ –æ—Ä–∫–µ—Å—Ç—Ä–∞, —Ç–æ –ë–µ—Ä–∏–æ —à–ª–µ—Ç –ë–∞—Ö—É –ø–æ–∫–ª–æ–Ω —É–∂–µ –ø–æ–¥—á–µ—Ä–∫–Ω—É—Ç–æ –∏–∑ –Ω–∞—à–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏, –∑–∞–¥–µ–π—Å—Ç–≤—É—è —Å—Ä–µ–¥–∏ –ø—Ä–æ—á–µ–≥–æ –∞—Ä—Ñ—É –∏ –¥–≤–∞ —Å–∞–∫—Å–æ—Ñ–æ–Ω–∞. ¬´–í —Ç–æ–º, –∫–∞–∫ –∏–∑—è—â–Ω–æ –ë–µ—Ä–∏–æ –∑–∞–≤–µ—Ä—à–∞–µ—Ç –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π –ö–æ–Ω—Ç—Ä–∞–ø—É–Ω–∫—Ç –ë–∞—Ö–∞, –ø—Ä–æ—è–≤–ª—è–µ—Ç—Å—è –µ–≥–æ –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ–µ –∫–æ–º–ø–æ–∑–∏—Ç–æ—Ä—Å–∫–æ–µ —á—É—Ç—å–µ –∏ –≤–∫—É—Å,‚Äî –≥–æ–≤–æ—Ä–∏—Ç –§–∏–ª–∏–ø–ø –ß–∏–∂–µ–≤—Å–∫–∏–π.‚Äî –ë–µ—Ä–∏–æ —É–¥–∞–ª–æ—Å—å –±—É–∫–≤–∞–ª—å–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∞–∫–∫–æ—Ä–¥–∞–º–∏ –æ–±—Ä–∞–º–∏—Ç—å —Ç–æ, —á—Ç–æ —Å–¥–µ–ª–∞–ª –ë–∞—Ö, —Å —á–∏—Å—Ç–æ –∏—Ç–∞–ª—å—è–Ω—Å–∫–∏–º –≤–∫—É—Å–æ–º. –ë–µ—Ä–∏–æ —Å—É–º–µ–ª –≤—ã–π—Ç–∏ –∏–∑ –ë–∞—Ö–∞, –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –º–µ–Ω—è—è, –ø—Ä–µ–ª–æ–º–ª—è—è –µ–≥–æ —Ç–∞–∫, —á—Ç–æ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –∫–∞–∫–æ–µ-—Ç–æ –∫–æ—Å–º–∏—á–µ—Å–∫–æ–µ —Å—É–º–∞—Å—à–µ—Å—Ç–≤–∏–µ. –í —Ö–æ—Ä–æ—à–µ–º —Å–º—ã—Å–ª–µ¬ª. –û–±–∞ –ø–µ—Ä–µ–ª–æ–∂–µ–Ω–∏—è –≤–¥–æ–±–∞–≤–æ–∫ –Ω–∞ —Ä–µ–¥–∫–æ—Å—Ç—å –ø–æ–¥–æ—à–ª–∏ –ü—è—Ç–Ω–∞–¥—Ü–∞—Ç–æ–π —Å–∏–º—Ñ–æ–Ω–∏–∏ ‚Äî –µ—â–µ –æ–¥–Ω–æ–º—É –¥–∏–∞–ª–æ–≥—É —Å –≤–µ–ª–∏–∫–∏–º–∏ –ø—Ä–µ–¥—à–µ—Å—Ç–≤–µ–Ω–Ω–∏–∫–∞–º–∏, –≤ —Ç–æ–º —á–∏—Å–ª–µ –ø–æ–∑–¥–Ω–µ–≥–æ –®–æ—Å—Ç–∞–∫–æ–≤–∏—á–∞ —Å –º–æ–ª–æ–¥—ã–º.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gXVpef-eo0O1"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω–∏–º –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –≤ .txt —Ñ–∞–π–ª \n",
        "train_path = 'train_dataset.txt'\n",
        "with open(train_path, \"w\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "train_dataset = TextDataset( tokenizer=tokenizer,file_path=train_path,block_size=64, \n",
        "                            overwrite_cache=True)\n",
        "  \n",
        "# —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –ø–æ–¥–∞–≤–∞—Ç—å –≤ –º–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã–µ –≤ –Ω—É–∂–Ω–æ–º –µ–π –≤–∏–¥–µ\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soPxrXgQpFZQ",
        "outputId": "4ad4e925-7b66-40a4-8375-02b5fc91ef5e"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "Creating features from dataset file at \n",
            "Saving features into cached file cached_lm_GPT2Tokenizer_64_train_dataset.txt [took 0.000 s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –û–±—É—á–µ–Ω–∏–µ\n"
      ],
      "metadata": {
        "id": "l_8m7ZLOpPCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments( \n",
        "    output_dir= \"./finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=100, \n",
        "    per_device_train_batch_size=32, \n",
        "    per_device_eval_batch_size=32,  \n",
        "    gradient_accumulation_steps=16, \n",
        "    )\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-5),None) # Optimizer and lr scheduler\n",
        ")"
      ],
      "metadata": {
        "id": "sQH4wBW_pN2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44c2db0e-cbdc-44b5-d7d6-42d8ec555517"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "OVtvq4sLq9EG",
        "outputId": "64185abf-9d30-4deb-95a9-02f4d83fa9b0"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 15\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 100\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 00:28, Epoch 100/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=100, training_loss=0.10304880142211914, metrics={'train_runtime': 28.7568, 'train_samples_per_second': 52.162, 'train_steps_per_second': 3.477, 'total_flos': 48992256000000.0, 'train_loss': 0.10304880142211914, 'epoch': 100.0})"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"–ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ, —Å –µ–≥–æ —à—É—Ç–ª–∏–≤–æ–π —Ü–∏—Ç–∞—Ç–æ–π –∏–∑ –†–æ—Å—Å–∏–Ω–∏ –≤ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏ –∏ –∏—Å–∫–∞–∂–µ–Ω–Ω–æ–π —Ü–∏—Ç–∞—Ç–æ–π –∏–∑ –í–∞–≥–Ω–µ—Ä–∞ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π, \\\n",
        " –≤—ã–∑–≤–∞–ª–æ –æ–∑–∞–¥–∞—á–µ–Ω–Ω—É—é —Ä–µ–∞–∫—Ü–∏—é —É –≤—Å–µ—Ö.\"\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model.generate(input_ids, \n",
        "                        do_sample=True,\n",
        "                        temperature=0.09,\n",
        "                        top_k=10,\n",
        "                        max_length=100,\n",
        "                        )\n",
        "\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_qjIcBUrq0j",
        "outputId": "5f6f04d3-e47c-43df-d26e-c5fc60986048"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "–ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ, —Å –µ–≥–æ —à—É—Ç–ª–∏–≤–æ–π —Ü–∏—Ç–∞—Ç–æ–π –∏–∑ –†–æ—Å—Å–∏–Ω–∏ –≤ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏ –∏ –∏—Å–∫–∞–∂–µ–Ω–Ω–æ–π —Ü–∏—Ç–∞—Ç–æ–π –∏–∑ –í–∞–≥–Ω–µ—Ä–∞ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π,  –≤—ã–∑–≤–∞–ª–æ –æ–∑–∞–¥–∞—á–µ–Ω–Ω—É—é —Ä–µ–∞–∫—Ü–∏—é —É –≤—Å–µ—Ö. ¬´–ë–µ—Ç—Ö–æ–≤–µ–Ω –Ω–∞–ø–∏—Å–∞–ª –¥–ª—è —Ñ–æ—Ä—Ç–µ–ø–∏–∞–Ω–æ —Å–∏–º—Ñ–æ–Ω–∏—á–µ—Å–∫—É—é –º—É–∑—ã–∫—É¬ª ‚Äî —Å –Ω–µ–¥–æ—É–º–µ–Ω–∏–µ–º –≤–æ—Å–∫–ª–∏–∫–Ω—É–ª–∏ –º–Ω–æ–≥–∏–µ. –ê –º–µ–∂–¥—É —Ç–µ–º —Ä–µ—á—å –∏–¥–µ—Ç –æ –ë–æ–ª—å—à–æ–º —Å–∏–º—Ñ–æ–Ω–∏—á–µ—Å–∫–æ–º –æ—Ä–∫–µ—Å—Ç—Ä–µ –ø–æ–¥ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ–º –ú–æ—Ü–∞—Ä—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –≤ —ç—Ç–æ–º –≥–æ–¥—É –æ—Ç–º–µ—á–∞–µ—Ç —Å–≤–æ–µ –¥–≤–∞–¥—Ü–∞—Ç–∏–ª–µ—Ç–∏–µ. –ö–∞–∫ –≤—Å–ø–æ–º–∏–Ω–∞–µ—Ç –º—É–∑—ã–∫–æ–≤–µ–¥ –†–∏—á–∞—Ä–¥ –ë–∏–∫–æ–Ω—Å,  –≤ —Ç–µ –≥–æ–¥—ã  –ú–æ—Ü–∞—Ä\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**–ó–∞–¥–∞–Ω–∏–µ 2 (2 –±–∞–ª–ª–∞)**\n",
        "\n",
        "*–û—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã:* \n",
        "\n",
        "*1) –í –∫–∞–∫–∏—Ö —Å—Ç–∞—Ç—å—è –±—ã–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã GPT-1, GPT-2, GPT-3?*\n",
        "\n",
        "*2) –ö–∞–∫ —Å–æ–±–∏—Ä–∞–ª—Å—è –æ–±—É—á–∞—é—â–∏–π –∫–æ—Ä–ø—É—Å –¥–ª—è GPT-3? –ö–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º —Å–æ–∑–¥–∞—Ç–µ–ª–∏ —Å—Ç–∞—Ä–∞–ª–∏—Å—å –æ–±–µ—Å–ø–µ—á–∏—Ç—å –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ?*"
      ],
      "metadata": {
        "id": "RSUiqScqWJJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) –ü–µ—Ä–≤–∞—è –≤–µ—Ä—Å–∏—è GPT (Generative Pre-trained Transformer) –æ—Ç OpenAI –ø–æ—è–≤–∏–ª–∞—Å—å –≤ 2018 –≥–æ–¥—É –∏ –æ–ø–∏—Å–∞–Ω–∞ –≤ (Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding by generative pre-training). GPT-2 –æ–ø–∏—Å–∞–Ω–∞ –≤ (Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8), 9.) GPT-3 –æ–ø–∏—Å–∞–Ω–∞ –≤ (Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33, 1877-1901.)"
      ],
      "metadata": {
        "id": "LKVqmIlEWRSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) –ó–∞ –æ—Å–Ω–æ–≤—É –±—ã–ª –≤–∑—è—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Common Crawl2 (–ø–æ—á—Ç–∏ —Ç—Ä–∏–ª–ª–∏–æ–Ω —Å–ª–æ–≤). –≠—Ç–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–∞–º—ã—Ö –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π. –û–¥–Ω–∞–∫–æ —Å–æ–∑–¥–∞—Ç–µ–ª–∏ GPT-3 –≤—ã—è—Å–Ω–∏–ª–∏, —á—Ç–æ –Ω–µ—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–ª–∏ —Å–ª–∞–±–æ—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ Common Crawl, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –∏–º–µ—é—Ç –±–æ–ª–µ–µ –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, —á–µ–º –±–æ–ª–µ–µ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö.\n",
        "–ß—Ç–æ–±—ã —É–ª—É—á—à–∏—Ç—å —Å—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –æ–Ω–∏ —Å–¥–µ–ª–∞–ª–∏ —Å–ª–µ–¥—É—é—â–µ–µ: \n",
        "\n",
        "(1) –∑–∞–≥—Ä—É–∑–∏–ª–∏ –∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–ª–∏ –≤–µ—Ä—Å–∏—é CommonCrawl –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ —Å —Ä—è–¥–æ–º –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–ø—Ä–∞–≤–æ—á–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤;\n",
        "\n",
        "(2) –≤—ã–ø–æ–ª–Ω–∏–ª–∏ –Ω–µ—á–µ—Ç–∫—É—é –¥–µ–¥—É–±–ª–∏–∫–∞—Ü–∏—é –Ω–∞ —É—Ä–æ–≤–Ω–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –≤–Ω—É—Ç—Ä–∏ –∏ –º–µ–∂–¥—É –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏, —á—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏;\n",
        "\n",
        "(3) –¥–æ–±–∞–≤–∏–ª–∏ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –∫–æ—Ä–ø—É—Å–∞ –≤ –Ω–∞–±–æ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.\n"
      ],
      "metadata": {
        "id": "1TrguJ1TWXPS"
      }
    }
  ]
}