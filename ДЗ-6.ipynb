{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9372007d",
   "metadata": {},
   "source": [
    "### Задание 2 (3 балла)\n",
    "Обучите 1 word2vec и 1 fastext модель в gensim. В каждой из модели нужно задать все параметры, которые мы разбирали на семинаре. Заданные значения должны отличаться от дефолтных и от тех, что мы использовали на семинаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5b79364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67bf6bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = open('wiki_data.txt', encoding='utf8').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ccb79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = text.lower().split()\n",
    "    tokens = [token.strip(punctuation) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbcfe18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [preprocess(text) for text in wiki]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ec68a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v = gensim.models.Word2Vec(texts, \n",
    "                             vector_size=400, \n",
    "                             min_count=50, \n",
    "                             max_vocab_size=20000,\n",
    "                             window=4,\n",
    "                             epochs=10,\n",
    "                             sg=1, \n",
    "                             hs=0,\n",
    "                             negative=12,\n",
    "                             sample=1e-4,\n",
    "                             ns_exponent=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed9f3633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('двигателем', 0.5316390991210938),\n",
       " ('шасси', 0.5194934606552124),\n",
       " ('двигатель', 0.4993988275527954),\n",
       " ('модель', 0.4937862157821655),\n",
       " ('мощность', 0.48875167965888977),\n",
       " ('подвеска', 0.483656644821167),\n",
       " ('машину', 0.46757638454437256),\n",
       " ('автомобиль', 0.4543384611606598),\n",
       " ('колёс', 0.4447077810764313),\n",
       " ('автомобиля', 0.43612030148506165)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar('машина')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0860f6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ft = gensim.models.FastText(texts, \n",
    "                            vector_size=500, \n",
    "                            min_count=50, \n",
    "                            max_vocab_size=20000,\n",
    "                            window=6,\n",
    "                            epochs=10, \n",
    "                            hs=1,\n",
    "                            cbow_mean=1,\n",
    "                            min_n=4, \n",
    "                            max_n=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad046cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('машину', 0.6642086505889893),\n",
       " ('машин', 0.6501579284667969),\n",
       " ('машины', 0.6370182633399963),\n",
       " ('модель', 0.44380733370780945),\n",
       " ('мощность', 0.4420733153820038),\n",
       " ('автомобиля', 0.4212818145751953),\n",
       " ('подвеска', 0.4182339608669281),\n",
       " ('двигателем', 0.4160038232803345),\n",
       " ('шасси', 0.4043676257133484),\n",
       " ('версия', 0.3960721790790558)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.wv.most_similar('машина')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06e400",
   "metadata": {},
   "source": [
    "### Задание 1 (3 балла)\n",
    "Обучите word2vec модели с негативным семплированием (cbow и skip-gram) с помощью tensorflow аналогично тому, как это было сделано в семинаре. Вам нужно изменить следующие пункты: 1) добавьте лемматизацию в предобработку (любым способом)\n",
    "2) измените размер окна на 6 для cbow и 12 для skip gram (обратите внимание, что размер окна = #слов слева + #слов справа, в gen_batches в семинаре window не так используется)\n",
    "3) измените часть с np.random.randint(vocab_size) так, чтобы случайные негативные примеры выбирались обратно пропорционально частотностям слов (частотные должны выбираться реже, а редкие чаще)\n",
    "\n",
    "Выберете несколько не похожих по смыслу слов, и протестируйте полученные эмбединги (найдите ближайшие слова и оцените правильность, как в семинаре)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "033a746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = open('wiki_data.txt', encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13441ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee373412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e3714f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = text.lower().split()\n",
    "    tokens = [token.strip(punctuation) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c32556",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = preprocess(wiki)\n",
    "words = ', '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e357df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_analized = [morph.parse(token) for token in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa9019",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph.parse(tokens)[0].normal_form"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
