{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59295ab5",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов).\n",
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти.\n",
    "\n",
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста. Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "Подсказки:\n",
    "\n",
    "- нужно будет добавить еще один тэг <start>  \n",
    "- еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы  \n",
    "- тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c03d0409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b319454",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = open('lenta.txt', encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea5a24ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "69160cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_news = normalize(news)\n",
    "norm_news, norm_news_test = norm_news[:10000], norm_news[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "77afd2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_news = Counter(norm_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e5866690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "def words_to_ngrams(words, n = 3, sep=\" \"):\n",
    "    return [sep.join(words[i:i+n]) for i in range(len(words)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7af88e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_news = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "069d2531",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n",
    "    trigrams_news.update(words_to_ngrams(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "09acbe09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start> <start>', 76344),\n",
       " ('<start> в', 7972),\n",
       " ('<start> по', 6211),\n",
       " ('<start> как', 3738),\n",
       " ('риа новости', 3504),\n",
       " ('по словам', 1971),\n",
       " ('об этом', 1795),\n",
       " ('<start> однако', 1694),\n",
       " ('<start> на', 1643),\n",
       " ('что в', 1624)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_news.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f2e13978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start> <start> в', 7972),\n",
       " ('<start> <start> по', 6211),\n",
       " ('<start> <start> как', 3738),\n",
       " ('<start> <start> однако', 1694),\n",
       " ('<start> <start> на', 1643),\n",
       " ('<start> <start> об', 1619),\n",
       " ('<start> об этом', 1579),\n",
       " ('<start> <start> он', 1553),\n",
       " ('<start> по словам', 1549),\n",
       " ('сообщает риа новости', 1324)]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams_news.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3feea8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, csc_matrix, lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "da775b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_news = lil_matrix((len(bigrams_news), \n",
    "                   len(unigrams_news)))\n",
    "\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "\n",
    "id2bigram_news = list(bigrams_news)\n",
    "bigram2id_news = {word:i for i, word in enumerate(id2bigram_news)}\n",
    "\n",
    "for ngram in trigrams_news:\n",
    "    word1, word2, word3 = ngram.split(' ')\n",
    "    bigram = word1 + ' ' + word2\n",
    "    matrix_news[bigram2id_news[bigram], word2id_news[word3]] = (trigrams_news[ngram]/bigrams_news[bigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9a58c791",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_news = csr_matrix(matrix_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "698c7d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2bigram, id2word, bigram2id, n=200, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = bigram2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(list(range(matrix.shape[1])), p=matrix[current_idx].toarray()[0])\n",
    "        \n",
    "        text.append(id2word[chosen])\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            current_idx = bigram2id[start]\n",
    "        \n",
    "        else:\n",
    "            part = id2bigram[current_idx] + ' ' + id2word[chosen]\n",
    "            part = ' '.join(part.split()[1:])\n",
    "            current_idx = bigram2id[part]\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "104a89a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "по данным командования части в которой in означает intelligence разведка it информационные технологии крайне важный аспект нашей стабильности развития нации а равно и петербургском метро вероятно будет третьим членом экипажа наряду с сша о предоставлении россии очередного кредитного транша по 50 миллионов долларов и специалисты мчс \n",
      " одна из электростанций компании \n",
      " мы практически завершим эту работу следует по его словам полевой командир хамзат батаев считавшийся командующим бамутским направлением сопротивления чеченских боевиков \n",
      " по состоянию на 13.00 московского времени в воскресенье вечером в воскресенье 26 декабря конкурса красоты семь красавиц \n",
      " таким образом мнение писателей представляет собой самую крупную с 1980-х годов бессрочную забастовку сообщает риа новости путин прибыл в псков в ближайшие полтора года рейтинг столичного градоначальника \n",
      " в этом регионе и не меньше месяца \n",
      " при задержании бабицкого \n",
      " напомним что вчера президента не может быть приравнена к военным мерам можно отнести церемонию возложения венков к местам захоронения советских воинов \n",
      " напомним что юрий лужков заявил журналистам что в декабре этого года товарооборот между россией и белоруссией \n",
      " предложение виктора илюхина \n",
      " около полудня сторонники станислава дерева перекрыли автомагистраль черкесск-ставрополь \n",
      " как утверждал джамалов он не исключил что на данный момент никаких законодательно на подобные вычисления могли иметь\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2bigram_news, id2word_news, bigram2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c5534107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "с другими членами руководства совета европы во главе с исполнительным секретарем снг юрием яровым сказал что на прошедшем в понедельник носит так как является налоговой тайной \n",
      " по данным следствия ялькин установил программу отслеживающую ответы информационной системы таможенных органов \n",
      " таким образом держать в напряжении республику \n",
      " по мнению главы цик альбом существует в природе \n",
      " что же касается потерь федеральных сил чьи интересы могут затрагивать зарегистрированные на острове сан мигель \n",
      " по словам представителей фсб финансовые средства официально направляемые на социальные нужды \n",
      " председатель испанского правительства хосе мариа аснаром \n",
      " российские военные обнаружили наблюдательный пост боевиков \n",
      " крупнейший американский провайдер aol активно рекламировал эту программу уверяя что она никогда его ни к чему не привели \n",
      " все свидетельствовало о том что под его командованием \n",
      " геннадий лузин был избран губернатором красноярского края проверяет причастность нескольких десятков стивен кинг набравшись опыта торговли электронными книгами подразумеваются устройства специально созданные для чтения текстов можно использовать матерные слова из типичного лексикона расистов сообщает washington post \n",
      " укрепление жеадминистративной границы нужно но экономически мы сейчас наблюдаем как 11 кандидатов одним из кандидатов борис терпугов недавно был назначен ошибочно так как расположено где-то внизу страницы среди обзоров \n",
      " об этом в петербурге у провайдера америки\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2bigram_news, id2word_news, bigram2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "96435c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "однако как полагают депутаты утвержденный правительством список товаров уже существенно сократился \n",
      " об этом итар-тасс сообщили сегодня в гудермес грозный шали урус-мартан и другие нефтяные компании в районе села шатили сообщает риа новости администрация клинтона не будет \n",
      " отвечая на вопрос зачем ты это сказал \n",
      " площадь пожара достигала 200 квадратных метров на складе сдетонировала еще одна его встреча с верховным комиссаром оон по делам беженцев призвал российские власти чинить им не исполнится 19 лет от рака лимфатических сосудов который был подписан \n",
      " об этом он заявил что огульные обвинения не делают чести ихавторам \n",
      " для этого подготовительную работу \n",
      " однако на прошлой неделе был также очень недоволен басаевым и удуговым свидетельствует о том что является владельцем легкового автомобиля мерседес-бенц-600 и дачи площадью 400 кв \n",
      " для попадания в восьмерку сильнейших попал кафельников обыгравший в воскресенье с международного плавучего космодрома морской старт успешно прошел клинические испытания во многом пользователи сами виноваты в убийстве их приговорят к пожизненному заключению \n",
      " отстаивавший позицию администрации министр финансов сша лоуренс саммерс на пресс-конференции заместитель начальника валерий манилов в ближайшее время сведения на этот сайт только для мужчины как то предписывается в разбросанных накануне над французской командой бордо в розыгрыше кубка стенли победитель определялся в\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2bigram_news, id2word_news, bigram2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11785f92",
   "metadata": {},
   "source": [
    "## Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14838d",
   "metadata": {},
   "source": [
    "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "Развернуто (в пределах 1000 знаков) ответьте на вопросы (по-русски):\n",
    "\n",
    "1.\tЧто можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?\n",
    "\n",
    "Система, которая использует слова, не входящие в словарь, называется системой с открытым словарем (open vocabulary system). В тестовую выборку добавляется псевдослово UNK. Есть два способа предсказать его вероятность.\n",
    "\n",
    "1)Превратить систему с открытым словарем в систему с закрытым словарем. Для этого нужно выбрать зафиксированный список слов (словарь). В обучающей выборке каждое несловарное слово конвертировать в токен UNK на этапе нормализации. Оценить вероятность для UNK так же, как для любого другого слова в обучающей выборке. \n",
    "\n",
    "2)Если словаря нет, то такой словарь можно создать, заменив слова на токен UNK, основываясь на частотности. Например, мы заменяем на UNK все слова, которые встречаются реже, чем n раз, где n – некоторое небольшое число. Или же задать словарь размером V с запасом (например, 50 тысяч слов), выбрать V самых частотных слов, а остальные заменить на UNK. В любом из этих случаев мы продолжаем обучать языковую модель, как и раньше, рассматривая UNK как обычное слово.\n",
    "\n",
    "Выбор модели влияет на показатель перплексии.\n",
    "\n",
    "    2.\tЧто такое сглаживание (smoothing)?\n",
    "\n",
    "Слова, которые есть в словаре, могут появиться в тестовой выборке в неизвестном контексте (например, после того слова, после которого они никогда не появлялись при обучении). Чтобы языковая модель не приписывала таким событиям нулевую вероятность, нам нужно сгладить вероятности: повысить вероятности некоторых n-грамм за счет понижения вероятности других. Есть несколько методов сглаживания: сглаживание Лапласа, сглаживание add-k, откат и сглаживание Кнезера-Нея.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2face1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
